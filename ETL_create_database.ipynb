{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "from config import db_password\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in c:\\python\\lib\\site-packages (2.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Kaggle metadata is cleaned (4 pt)\n",
    "def clean_movie(movie):\n",
    "    movie = dict(movie) #create a non-destructive copy\n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Add the function that takes in three arguments;\n",
    "# Wikipedia data, Kaggle metadata, and MovieLens rating data (from Kaggle)\n",
    "# Before Step 2, add all the code written for Deliverable 2\n",
    "\n",
    "# 2 Add the function that takes in three arguments; Wikipedia data, Kaggle metadata, and MovieLens rating data (from Kaggle)\n",
    "# The Wikipedia and Kaggle DataFrames are merged (3 pt)\n",
    "\n",
    "def extract_transform_load(wiki_file, kaggle_file, ratings_file):    \n",
    "    \n",
    "    file_dir = 'C://Users/KenAk/ETL/Movies-ETL/'\n",
    "    \n",
    "    # Read in the kaggle metadata and MovieLens ratings CSV files as Pandas DataFrames.\n",
    "\n",
    "    kaggle_metadata = pd.read_csv(f'{file_dir}movies_metadata.csv', low_memory=False)\n",
    "    ratings = pd.read_csv(f'{file_dir}ratings.csv')\n",
    "    \n",
    "    # Open and read the Wikipedia data JSON file.\n",
    " \n",
    "    with open(f'{file_dir}/wikipedia-movies.json', mode='r') as file:\n",
    "        wiki_movies_raw = json.load(file)\n",
    "    \n",
    "    # 3. Write a list comprehension to filter out TV shows. Code from near bottom of 8.3.3\n",
    "    # The TV shows are filtered out, and the wiki_movies_df DataFrame is created (3 pt)\n",
    "    \n",
    "    wiki_movies = [movie for movie in wiki_movies_raw\n",
    "                   if ('Director' in movie or 'Directed by' in movie)\n",
    "                       and 'imdb_link' in movie\n",
    "                       and 'No. of episodes' not in movie]\n",
    "    \n",
    "    # 4. Write a list comprehension to iterate through the cleaned wiki movies list and call the clean_movie function on each movie.\n",
    "    # We can make a list of cleaned movies with a list comprehension\n",
    "    \n",
    "    clean_movies = [clean_movie(movie) for movie in wiki_movies]\n",
    "\n",
    "    # 5. Read in the cleaned movies list from Step 4 as a DataFrame.\n",
    "\n",
    "    wiki_movies_df = pd.DataFrame(clean_movies)\n",
    "    \n",
    "    # 6. Write a try-except block to catch errors while extracting the IMDb ID using a regular expression string and\n",
    "    #  dropping any imdb_id duplicates. If there is an error, capture and print the exception.\n",
    "    # A try-except block is used to catch errors while extracting the IMDb IDs with a regular expression and dropping duplicate IDs. (5 pt)\n",
    "    try:\n",
    "        wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "        print(\"Number of movies before dropping duplicates:\", len(wiki_movies_df))\n",
    "        wiki_movies_df.drop_duplicates(subset='imdb_id', inplace=True)\n",
    "        print(\"Number of movies after dropping duplicates: \",len(wiki_movies_df))\n",
    "    except:\n",
    "        print(\"An exception occurred\")\n",
    "\n",
    "    #  7. Write a list comprehension to keep the columns that don't have null values from the wiki_movies_df DataFrame.\n",
    "    # That will give us the columns that we want to keep, which we can select from our Pandas DataFrame\n",
    "    # A list comprehension is used to keep columns with non-null values (3 pt)\n",
    "    wiki_columns_to_keep = [column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9]\n",
    "    wiki_movies_df = wiki_movies_df[wiki_columns_to_keep]\n",
    "     \n",
    "    # 8. Create a variable that will hold the non-null values from the “Box office” column.\n",
    "    # The non-null box office data is converted to string values using the lambda and join functions (3 pt)\n",
    "\n",
    "    Box_Office = wiki_movies_df['Box office'].dropna() #drop missing values\n",
    "    \n",
    "    # 9. Convert the box office data created in Step 8 to string values using the lambda and join functions.  \n",
    "    # Lambda functions don't have a name and automatically return a variable\n",
    "    Box_Office[Box_Office.map(lambda x: type(x) != str)]\n",
    "    # Instead of creating a new function with a block of code and the def keyword, we can create an anonymous lambda function right inside the map() call\n",
    "\n",
    "    # 10. Write a regular expression to match the six elements of \"form_one\" of the box office data.  \n",
    "    # 8.3.10 Parse the Box Office Data; A regular expression is used to match the six elements of \"form_one\" of the box office data (2 pt)\n",
    "    form_one = r'\\$\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "    Box_Office.str.contains(form_one, flags=re.IGNORECASE, na=False).sum()\n",
    "    \n",
    "    # 11. Write a regular expression to match the three elements of \"form_two\" of the box office data.\n",
    "    # A regular expression is used to match the three elements of \"form_two\" of the box office data (2 pt)\n",
    "    form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illi?on)'\n",
    "    Box_Office.str.contains(form_two, flags=re.IGNORECASE, na=False).sum()\n",
    "    \n",
    "    # 12. Add the parse_dollars function.\n",
    "    def parse_dollars(s):\n",
    "        # if s is not a string, return NaN\n",
    "        if type(s) != str:\n",
    "            return np.nan\n",
    "\n",
    "        # if input is of the form $###.# million\n",
    "        if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "            # remove dollar sign and \" million\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "            # convert to float and multiply by a million\n",
    "            value = float(s) * 10**6\n",
    "\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # if input is of the form $###.# billion\n",
    "        elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "            # remove dollar sign and \" billion\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "            # convert to float and multiply by a billion\n",
    "            value = float(s) * 10**9\n",
    "\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # if input is of the form $###,###,###\n",
    "        elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illi?on)', s, flags=re.IGNORECASE):\n",
    "\n",
    "            # remove dollar sign and commas\n",
    "            s = re.sub('\\$|,','', s)\n",
    "\n",
    "            # convert to float\n",
    "            value = float(s)\n",
    "\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # otherwise, return NaN\n",
    "        else:\n",
    "            return np.nan\n",
    "    \n",
    "    # The following columns are cleaned in the Wikipedia DataFrame: (8 pt)\n",
    "    \n",
    "    # The box office column\n",
    "    # The budget column\n",
    "    # The release date column\n",
    "    # The running time column\n",
    "    \n",
    "    # 13. Clean the box office column in the wiki_movies_df DataFrame.\n",
    "\n",
    "    wiki_movies_df['Box_Office'] = Box_Office.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    wiki_movies_df.drop('Box office', axis=1, inplace=True)\n",
    "    \n",
    "    # 14. Clean the budget column in the wiki_movies_df DataFrame.\n",
    "    \n",
    "    budget = wiki_movies_df['Budget'].dropna()\n",
    "    budget = budget.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "    matches_form_one = budget.str.contains(form_one, flags=re.IGNORECASE, na=False)\n",
    "    matches_form_two = budget.str.contains(form_two, flags=re.IGNORECASE, na=False)\n",
    "    budget[~matches_form_one & ~matches_form_two]\n",
    "    # Remove the citation references with the following:\n",
    "    budget = budget.str.replace(r'\\[\\d+\\]\\s*', '')\n",
    "    budget[~matches_form_one & ~matches_form_two]\n",
    "    # make a variable that holds the non-null values of Release date in the DataFrame, converting lists to strings\n",
    "    wiki_movies_df['budget'] = budget.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    # We can also drop the original Budget column\n",
    "    wiki_movies_df.drop('Budget', axis=1, inplace=True)\n",
    "    \n",
    "    # 15. Clean the release date column in the wiki_movies_df DataFrame.\n",
    "    \n",
    "    # make a variable that holds the non-null values of Release date in the DataFrame, converting lists to strings\n",
    "\n",
    "    Release_Date = wiki_movies_df['Release date'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    # The forms we'll be parsing are:\n",
    "\n",
    "    # 1. Full month name, one- to two-digit day, four-digit year (i.e., January 1, 2000)\n",
    "    # 2. Four-digit year, two-digit month, two-digit day, with any separator (i.e., 2000-01-01)\n",
    "    # 3. Full month name, four-digit year (i.e., January 2000)\n",
    "    # 4. Four-digit year\n",
    "\n",
    "    date_form_one = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s[123]?\\d,\\s\\d{4}'\n",
    "    date_form_two = r'\\d{4}.[01]\\d.[0123]\\d'\n",
    "    date_form_three = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}'\n",
    "    date_form_four = r'\\d{4}'\n",
    "    \n",
    "    # Extract the dates\n",
    "\n",
    "    Release_Date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})', flags=re.IGNORECASE)\n",
    "    wiki_movies_df['Release_Date'] = pd.to_datetime(Release_Date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})')[0], infer_datetime_format=True)\n",
    "    # We can also drop the original Release date column\n",
    "    wiki_movies_df.drop('Release date', axis=1, inplace=True)\n",
    "    \n",
    "    # 16. Clean the running time column in the wiki_movies_df DataFrame.\n",
    "    # Parse running time\n",
    "    Running_Time = wiki_movies_df['Running time'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    running_time_extract = Running_Time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')\n",
    "    running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)\n",
    "    wiki_movies_df['Running_Time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)\n",
    "    wiki_movies_df.drop('Running time', axis=1, inplace=True)\n",
    "    \n",
    "    # Return three variables. The first is the wiki_movies_df DataFrame  \n",
    "    #return wiki_movies_df, kaggle_metadata, ratings \n",
    "\n",
    "    # Read in the kaggle metadata and MovieLens ratings CSV files as Pandas DataFrames.\n",
    "    \n",
    "    # Open and read the Wikipedia data JSON file.\n",
    "      \n",
    "    # Write a list comprehension to filter out TV shows.  \n",
    "\n",
    "    # Write a list comprehension to iterate through the cleaned wiki movies list and call the clean_movie function on each movie.\n",
    " \n",
    "    # Read in the cleaned movies list from Step 4 as a DataFrame.\n",
    "\n",
    "    # Write a try-except block to catch errors while extracting the IMDb ID using a regular expression string and\n",
    "    # dropping any imdb_id duplicates. If there is an error, capture and print the exception.\n",
    "\n",
    "    #  Write a list comprehension to keep the columns that don't have null values from the wiki_movies_df DataFrame.   \n",
    "\n",
    "    # Create a variable that will hold the non-null values from the “Box office” column.\n",
    "    \n",
    "    # Convert the box office data created in Step 8 to string values using the lambda and join functions.    \n",
    "\n",
    "    # Write a regular expression to match the six elements of \"form_one\" of the box office data.\n",
    "   \n",
    "    # Write a regular expression to match the three elements of \"form_two\" of the box office data.   \n",
    "\n",
    "    # Add the parse_dollars function.    \n",
    "        \n",
    "    # Clean the box office column in the wiki_movies_df DataFrame.\n",
    "    \n",
    "    # Clean the budget column in the wiki_movies_df DataFrame.\n",
    "    \n",
    "    # Clean the release date column in the wiki_movies_df DataFrame.\n",
    "    \n",
    "    # Clean the running time column in the wiki_movies_df DataFrame.    \n",
    "     \n",
    "    # 2. Clean the Kaggle metadata\n",
    "    \n",
    "    kaggle_metadata = kaggle_metadata[kaggle_metadata['adult'] == 'False'].drop('adult',axis='columns')\n",
    "    kaggle_metadata['video'] = kaggle_metadata['video'] == 'True'\n",
    "    kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "    kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "    kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise')\n",
    "    kaggle_metadata['release_date'] = pd.to_datetime(kaggle_metadata['release_date'])\n",
    "\n",
    "    # 3. Merged the two DataFrames into the movies DataFrame.\n",
    "    # 8.4.1 Merge Wikipedia and Kaggle Metadata\n",
    "    movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "\n",
    "    #Test code\n",
    "    movies_df.columns.to_list()\n",
    "    \n",
    "    # 4. Drop unnecessary columns from the merged DataFrame.\n",
    "    movies_df.drop(columns=['title_wiki','Language'], inplace=True)\n",
    "    \n",
    "    # 5. Add in the function to fill in the missing Kaggle data.\n",
    "    # Next, to save a little time, we'll make a function that fills in missing data for a column pair and then drops the redundant column\n",
    "    def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "             df[kaggle_column] = df.apply(\n",
    "                lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column], axis=1)\n",
    "             df.drop(columns=wiki_column, inplace=True)\n",
    "\n",
    "    # 6. Call the function in Step 5 with the DataFrame and columns as the arguments.\n",
    "    # Now we can run the function for the column pairs that we decided to fill in zeros\n",
    "    fill_missing_kaggle_data(movies_df, 'runtime', 'Running_Time')\n",
    "    fill_missing_kaggle_data(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "    fill_missing_kaggle_data(movies_df, 'revenue', 'Box_Office')\n",
    "    fill_missing_kaggle_data(movies_df, 'production_companies', 'Productioncompanies ')\n",
    "    fill_missing_kaggle_data(movies_df, 'release_date', 'Release_Date')\n",
    "    \n",
    "    # The above procedures, along with filtering and renaming movies_df DataFrame columns (see below), are worth 8 points.\n",
    "        \n",
    "    # 7. Filter the movies DataFrame for specific columns.\n",
    "    # Since we've merged our data and filled in values, it's good to check that there aren't any columns with only one value, since\n",
    "    # that doesn't really provide any information. Don't forget, we need to convert lists to tuples for value_counts() to work.\n",
    "\n",
    "    for col in movies_df.columns:\n",
    "        lists_to_tuples = lambda x: tuple(x) if type(x) == list else x\n",
    "        value_counts = movies_df[col].apply(lists_to_tuples).value_counts(dropna=False)\n",
    "        num_values = len(value_counts)\n",
    "        if num_values == 1:\n",
    "            movies_df.drop(columns=col, inplace=True)\n",
    "            print(\"We have dropped the\", col, \"column because it only has one value.\")\n",
    "                \n",
    "    # 8. Rename the columns in the movies DataFrame # Reorder, then rename the columns\n",
    "\n",
    "    movies_df = movies_df.loc[:, ['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "                       'runtime','budget_kaggle','revenue','release_date','popularity','vote_average','vote_count','genres',\n",
    "                       'original_language','overview','spoken_languages','Country','production_companies','production_countries',\n",
    "                       'Distributed by','Produced by','Directed by','Starring','Cinematography','Edited by','Written by','Screenplay by',\n",
    "                       'Music by','Based on','Productioncompany ','homepage'\n",
    "                      ]]\n",
    "\n",
    "    movies_df.rename({'id':'kaggle_id',\n",
    "                   'title_kaggle':'title',\n",
    "                   'url':'wikipedia_url',\n",
    "                   'budget_kaggle':'budget',\n",
    "                   'Productioncompany ':'production_company',\n",
    "                   'Country':'country',\n",
    "                   'Distributed by':'distributor',\n",
    "                   'Produced by':'producer',\n",
    "                   'Directed by':'director',\n",
    "                   'Starring':'starring',\n",
    "                   'Cinematography':'cinematography',\n",
    "                   'Edited by':'editors',\n",
    "                   'Written by':'writers',\n",
    "                   'Screenplay by':'screenplay',\n",
    "                   'Music by':'composers',\n",
    "                   'Based on':'based_on'\n",
    "                  }, axis='columns', inplace=True)\n",
    "    \n",
    "    # 9. Transform and merge the ratings DataFrame # 8.4.2 Transform and Merge Rating Data (10 points overall)\n",
    "    \n",
    "    # Add the code to create the connection to the PostgreSQL database, then add the movies_df DataFrame to a SQL database.\n",
    "    \n",
    "    rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count().rename({'userId':'count'}, axis=1) \\\n",
    "                .pivot(index='movieId',columns='rating', values='count') # (Clean 3 pt)\n",
    "    movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left') # (Merge 4 pt)\n",
    "    print(\"Number of movies with ratings:\",len(movies_with_ratings_df))\n",
    "    \n",
    "    #  We'll prepend rating_ to each column with a list comprehension:\n",
    "    rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                    .rename({'userId':'count'}, axis=1) \\\n",
    "                    .pivot(index='movieId',columns='rating', values='count')\n",
    "    rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "    \n",
    "    movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "\n",
    "    # Fill in missing values with zeros (3 pt)\n",
    "    movies_with_ratings_df[rating_counts.columns] = movies_with_ratings_df[rating_counts.columns].fillna(0)\n",
    "\n",
    "    #\"postgresql://[user]:[db_password]@[location]:[port]/[database]\"\n",
    "    db_string = f\"postgresql://postgres:{db_password}@127.0.0.1:5432/movie_data\"\n",
    "    \n",
    "    engine = create_engine(db_string)\n",
    "    \n",
    "    movies_df.to_sql(name='movies', con=engine)\n",
    "    \n",
    "    rows_imported = 0\n",
    "    # get the start_time from time.time()\n",
    "    start_time = time.time()\n",
    "    for data in pd.read_csv(f'{file_dir}ratings.csv', chunksize=1000000):\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "        data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "        rows_imported += len(data)\n",
    "\n",
    "        # add elapsed time to final print out\n",
    "        print(f'Done. {time.time() - start_time} total seconds elapsed')\n",
    "    \n",
    "    # Remove the return statement (Step 2)\n",
    "    # return wiki_movies_df, movies_with_ratings_df, movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Create the path to your file directory and variables for the three files.\n",
    "file_dir = 'C://Users/KenAk/ETL/Movies-ETL'\n",
    "# The Wikipedia data\n",
    "wiki_file = f'{file_dir}/wikipedia_movies.json'\n",
    "# The Kaggle metadata\n",
    "kaggle_file = f'{file_dir}/movies_metadata.csv'\n",
    "# The MovieLens rating data.\n",
    "ratings_file = f'{file_dir}/ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies before dropping duplicates: 7076\n",
      "Number of movies after dropping duplicates:  7033\n",
      "We have dropped the video column because it only has one value.\n",
      "Number of movies with ratings: 6052\n",
      "importing rows 0 to 1000000...Done. 78.13694977760315 total seconds elapsed\n",
      "importing rows 1000000 to 2000000...Done. 148.35907196998596 total seconds elapsed\n",
      "importing rows 2000000 to 3000000...Done. 218.6779317855835 total seconds elapsed\n",
      "importing rows 3000000 to 4000000...Done. 292.3657851219177 total seconds elapsed\n",
      "importing rows 4000000 to 5000000...Done. 373.9654657840729 total seconds elapsed\n",
      "importing rows 5000000 to 6000000...Done. 454.2576470375061 total seconds elapsed\n",
      "importing rows 6000000 to 7000000...Done. 532.7935273647308 total seconds elapsed\n",
      "importing rows 7000000 to 8000000...Done. 609.7865397930145 total seconds elapsed\n",
      "importing rows 8000000 to 9000000...Done. 686.6409142017365 total seconds elapsed\n",
      "importing rows 9000000 to 10000000...Done. 761.3819470405579 total seconds elapsed\n",
      "importing rows 10000000 to 11000000...Done. 837.3516926765442 total seconds elapsed\n",
      "importing rows 11000000 to 12000000...Done. 913.1159889698029 total seconds elapsed\n",
      "importing rows 12000000 to 13000000...Done. 989.5108819007874 total seconds elapsed\n",
      "importing rows 13000000 to 14000000...Done. 1063.9125485420227 total seconds elapsed\n",
      "importing rows 14000000 to 15000000...Done. 1138.388284444809 total seconds elapsed\n",
      "importing rows 15000000 to 16000000...Done. 1215.2109036445618 total seconds elapsed\n",
      "importing rows 16000000 to 17000000...Done. 1294.6740736961365 total seconds elapsed\n",
      "importing rows 17000000 to 18000000...Done. 1373.6104180812836 total seconds elapsed\n",
      "importing rows 18000000 to 19000000...Done. 1452.133143901825 total seconds elapsed\n",
      "importing rows 19000000 to 20000000...Done. 1532.1207842826843 total seconds elapsed\n",
      "importing rows 20000000 to 21000000...Done. 1611.5378484725952 total seconds elapsed\n",
      "importing rows 21000000 to 22000000...Done. 1690.6167342662811 total seconds elapsed\n",
      "importing rows 22000000 to 23000000...Done. 1769.5688545703888 total seconds elapsed\n",
      "importing rows 23000000 to 24000000...Done. 1847.9277057647705 total seconds elapsed\n",
      "importing rows 24000000 to 25000000...Done. 1928.015554189682 total seconds elapsed\n",
      "importing rows 25000000 to 26000000...Done. 2003.9674980640411 total seconds elapsed\n",
      "importing rows 26000000 to 26024289...Done. 2005.7302408218384 total seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "# 11. Set the three variables equal to the function created in D1.\n",
    "#wiki_file_returned, kaggle_file_returned, ratings_file_returned = extract_transform_load(wiki_file, kaggle_file, ratings_file)\n",
    "extract_transform_load(wiki_file, kaggle_file, ratings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Set the DataFrames from the return statement equal to the file names in Step 11. \n",
    "wiki_movies_df = wiki_file_returned\n",
    "movies_with_ratings_df = kaggle_file_returned\n",
    "movies_df = ratings_file_returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test code\n",
    "movies_with_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test code\n",
    "movies_with_ratings_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test code\n",
    "movies_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Check the wiki_movies_df DataFrame. \n",
    "wiki_movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Check the movies_with_ratings_df DataFrame.\n",
    "movies_with_ratings_df.loc[3513:3521]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Check the movies_df DataFrame\n",
    "movies_df.loc[3012:3040]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
